#!/usr/bin/env python3
"""
STEP NEXT-17B Audit Runner
All-Insurers Verification: Type Classification + Step7 Miss Detection + Regression Gates

Generates:
- docs/audit/AMOUNT_STATUS_DASHBOARD.md
- docs/audit/INSURER_TYPE_BY_EVIDENCE.md
- docs/audit/TYPE_MAP_DIFF_REPORT.md
- docs/audit/STEP7_MISS_CANDIDATES.md
"""

import json
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional
import PyPDF2

# Paths
PROJECT_ROOT = Path(__file__).parent.parent.parent
COMPARE_DIR = PROJECT_ROOT / "data" / "compare"
SOURCES_DIR = PROJECT_ROOT / "data" / "sources" / "insurers"
CONFIG_TYPE_MAP = PROJECT_ROOT / "config" / "amount_lineage_type_map.json"
AUDIT_DIR = PROJECT_ROOT / "docs" / "audit"

# Ensure audit dir exists
AUDIT_DIR.mkdir(parents=True, exist_ok=True)

# Type evidence patterns
TYPE_AB_PATTERNS = [
    re.compile(r'(보장명|담보명|보장내용|특약명).*(가입금액|보험가입금액).*(보험료|납입보험료)', re.IGNORECASE),
    re.compile(r'(\d{1,3}(,\d{3})*)\s*(만원|천만원|백만원)', re.IGNORECASE),
]

TYPE_C_PATTERNS = [
    re.compile(r'보험가입금액\s*[:：]\s*(\d{1,3}(,\d{3})*)\s*(만원|원)', re.IGNORECASE),
    re.compile(r'(보험가입금액|계약금액)\s*(의\s*)?\d+\s*%\s*지급', re.IGNORECASE),
    re.compile(r'보험가입금액\s*(한도|범위)', re.IGNORECASE),
]


def scan_insurers() -> List[str]:
    """Scan coverage_cards.jsonl files to get insurer list."""
    insurers = []
    for path in COMPARE_DIR.glob("*_coverage_cards.jsonl"):
        insurer = path.stem.replace("_coverage_cards", "")
        insurers.append(insurer)
    return sorted(insurers)


def parse_coverage_cards(insurer: str) -> List[dict]:
    """Parse coverage cards JSONL for a given insurer."""
    path = COMPARE_DIR / f"{insurer}_coverage_cards.jsonl"
    if not path.exists():
        return []

    cards = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                cards.append(json.loads(line))
            except json.JSONDecodeError:
                pass
    return cards


def generate_amount_status_dashboard(insurers: List[str]) -> str:
    """Generate AMOUNT_STATUS_DASHBOARD.md."""
    report_lines = [
        "# Amount Status Dashboard (All Insurers)",
        "",
        "**Generated by**: STEP NEXT-17B",
        "**Purpose**: Verify CONFIRMED/UNCONFIRMED/NOT_AVAILABLE distribution across all insurers",
        "",
        "## Summary Table",
        "",
        "| Insurer | CONFIRMED | UNCONFIRMED | NOT_AVAILABLE | Total | CONFIRMED % |",
        "|---------|-----------|-------------|---------------|-------|-------------|",
    ]

    totals = Counter()
    insurer_stats = {}

    for insurer in insurers:
        cards = parse_coverage_cards(insurer)
        stats = Counter()

        for card in cards:
            status = card.get("amount", {}).get("status", "NOT_AVAILABLE")
            stats[status] += 1
            totals[status] += 1

        total = sum(stats.values())
        confirmed = stats.get("CONFIRMED", 0)
        unconfirmed = stats.get("UNCONFIRMED", 0)
        not_available = stats.get("NOT_AVAILABLE", 0)
        confirmed_pct = (confirmed / total * 100) if total > 0 else 0.0

        insurer_stats[insurer] = stats

        report_lines.append(
            f"| {insurer} | {confirmed} | {unconfirmed} | {not_available} | {total} | {confirmed_pct:.1f}% |"
        )

    # Add total row
    total_all = sum(totals.values())
    total_confirmed = totals.get("CONFIRMED", 0)
    total_unconfirmed = totals.get("UNCONFIRMED", 0)
    total_not_available = totals.get("NOT_AVAILABLE", 0)
    total_confirmed_pct = (total_confirmed / total_all * 100) if total_all > 0 else 0.0

    report_lines.append(
        f"| **TOTAL** | **{total_confirmed}** | **{total_unconfirmed}** | **{total_not_available}** | **{total_all}** | **{total_confirmed_pct:.1f}%** |"
    )

    report_lines.extend([
        "",
        "## Interpretation",
        "",
        "- **CONFIRMED**: Amount successfully extracted with high confidence",
        "- **UNCONFIRMED**: Amount extraction attempted but low confidence or missing",
        "- **NOT_AVAILABLE**: No amount extraction attempted (coverage not in scope or no source)",
        "",
        "## Next Steps",
        "",
        "- Review UNCONFIRMED cases for potential Step7 miss candidates",
        "- Check Type classification accuracy for insurers with low CONFIRMED rates",
        "",
    ])

    return "\n".join(report_lines)


def extract_pdf_text(pdf_path: Path, max_pages: int = 10) -> List[Tuple[int, str]]:
    """Extract text from first max_pages of PDF. Returns [(page_num, text), ...]."""
    results = []
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            num_pages = min(len(reader.pages), max_pages)
            for i in range(num_pages):
                page = reader.pages[i]
                text = page.extract_text() or ""
                results.append((i + 1, text))
    except Exception:
        pass
    return results


def detect_type_from_pdf(insurer: str) -> Tuple[str, List[dict]]:
    """
    Detect insurer type from proposal PDF evidence.
    Returns: (type_code, evidence_list)
    type_code: "A/B", "C", "UNKNOWN"
    evidence: [{"page": int, "snippet": str, "pattern": str}, ...]
    """
    proposal_dir = SOURCES_DIR / insurer / "가입설계서"
    if not proposal_dir.exists():
        return ("UNKNOWN", [{"page": 0, "snippet": "NO_PROPOSAL_PDF", "pattern": "N/A"}])

    pdf_files = list(proposal_dir.glob("*.pdf"))
    if not pdf_files:
        return ("UNKNOWN", [{"page": 0, "snippet": "NO_PROPOSAL_PDF", "pattern": "N/A"}])

    # Use first PDF
    pdf_path = pdf_files[0]
    pages = extract_pdf_text(pdf_path, max_pages=10)

    ab_evidence = []
    c_evidence = []

    for page_num, text in pages:
        # Check Type A/B patterns
        for pattern in TYPE_AB_PATTERNS:
            matches = pattern.findall(text)
            if matches:
                snippet_start = text.find(matches[0] if isinstance(matches[0], str) else matches[0][0])
                snippet = text[snippet_start:snippet_start + 200].replace("\n", " ").strip()
                ab_evidence.append({
                    "page": page_num,
                    "snippet": snippet,
                    "pattern": pattern.pattern[:50]
                })
                break  # One evidence per page is enough

        # Check Type C patterns
        for pattern in TYPE_C_PATTERNS:
            matches = pattern.findall(text)
            if matches:
                snippet_start = text.find(matches[0] if isinstance(matches[0], str) else matches[0][0])
                snippet = text[snippet_start:snippet_start + 200].replace("\n", " ").strip()
                c_evidence.append({
                    "page": page_num,
                    "snippet": snippet,
                    "pattern": pattern.pattern[:50]
                })
                break

    # Deterministic rule: A/B evidence takes priority
    if ab_evidence:
        return ("A/B", ab_evidence[:3])  # Max 3 evidence items
    elif c_evidence:
        return ("C", c_evidence[:3])
    else:
        return ("UNKNOWN", [{"page": 0, "snippet": "NO_EVIDENCE_FOUND", "pattern": "N/A"}])


def generate_type_classification_report(insurers: List[str]) -> str:
    """Generate INSURER_TYPE_BY_EVIDENCE.md."""
    report_lines = [
        "# Insurer Type Classification by Evidence",
        "",
        "**Generated by**: STEP NEXT-17B",
        "**Method**: Document structure analysis from 가입설계서 PDFs",
        "",
        "## Type Definitions",
        "",
        "- **Type A/B**: Coverage-specific amounts listed in table format",
        "- **Type C**: Single 보험가입금액 reference, coverages refer to it",
        "- **UNKNOWN**: Insufficient evidence or no PDF available",
        "",
        "## Results by Insurer",
        "",
    ]

    type_results = {}

    for insurer in insurers:
        detected_type, evidence = detect_type_from_pdf(insurer)
        type_results[insurer] = (detected_type, evidence)

        report_lines.append(f"### {insurer}")
        report_lines.append(f"**Detected Type**: {detected_type}")
        report_lines.append("")
        report_lines.append("**Evidence**:")
        report_lines.append("")

        for ev in evidence:
            report_lines.append(f"- Page {ev['page']}: {ev['snippet'][:150]}...")
            report_lines.append(f"  - Pattern: `{ev['pattern']}`")
            report_lines.append("")

    report_lines.extend([
        "## Validation Notes",
        "",
        "- Evidence extraction is deterministic based on regex patterns",
        "- Only first 10 pages of first PDF scanned per insurer",
        "- Type A/B evidence takes priority over Type C evidence",
        "",
    ])

    return "\n".join(report_lines), type_results


def generate_type_map_diff_report(insurers: List[str], type_results: Dict[str, Tuple[str, List[dict]]]) -> str:
    """Generate TYPE_MAP_DIFF_REPORT.md comparing config vs evidence."""
    # Load config
    with open(CONFIG_TYPE_MAP, "r", encoding="utf-8") as f:
        config_map = json.load(f)

    report_lines = [
        "# Type Map Diff Report (Config vs Evidence)",
        "",
        "**Generated by**: STEP NEXT-17B",
        "**Config Source**: `config/amount_lineage_type_map.json`",
        "**Evidence Source**: PDF document structure analysis",
        "",
        "## Comparison Table",
        "",
        "| Insurer | Config Type | Evidence Type | Match? | Action Recommended |",
        "|---------|-------------|---------------|--------|-------------------|",
    ]

    diffs = []

    for insurer in insurers:
        config_type = config_map.get(insurer, "MISSING")
        evidence_type, _ = type_results.get(insurer, ("UNKNOWN", []))

        # Normalize for comparison: A/B in evidence matches A or B in config
        match = False
        if evidence_type == "A/B" and config_type in ("A", "B"):
            match = True
        elif evidence_type == config_type:
            match = True
        elif evidence_type == "UNKNOWN":
            match = None  # Cannot determine

        match_str = "✅" if match is True else ("❓" if match is None else "❌")
        action = "None" if match is True else ("Review evidence" if match is None else "Consider update")

        if match is False:
            diffs.append((insurer, config_type, evidence_type))

        report_lines.append(
            f"| {insurer} | {config_type} | {evidence_type} | {match_str} | {action} |"
        )

    report_lines.extend([
        "",
        "## Discrepancies",
        "",
    ])

    if diffs:
        for insurer, cfg, ev in diffs:
            report_lines.append(f"- **{insurer}**: Config={cfg}, Evidence={ev}")
        report_lines.append("")
        report_lines.append("**Recommendation**: Review evidence snippets in `INSURER_TYPE_BY_EVIDENCE.md` before updating config.")
    else:
        report_lines.append("No discrepancies detected. Config aligns with evidence.")

    report_lines.extend([
        "",
        "## Important",
        "",
        "- **DO NOT auto-update config** - manual review required",
        "- Evidence detection uses heuristic patterns and may have false positives",
        "- Final Type assignment must be validated by domain expert",
        "",
    ])

    return "\n".join(report_lines)


def detect_step7_miss_candidates(insurers: List[str]) -> Tuple[str, List[dict]]:
    """
    Detect Step7 miss candidates: UNCONFIRMED status but amount evidence in PDF.
    Returns: (report_markdown, candidates_list)
    """
    candidates = []

    for insurer in insurers:
        cards = parse_coverage_cards(insurer)
        proposal_dir = SOURCES_DIR / insurer / "가입설계서"

        if not proposal_dir.exists():
            continue

        pdf_files = list(proposal_dir.glob("*.pdf"))
        if not pdf_files:
            continue

        # Extract PDF text
        pdf_path = pdf_files[0]
        pages = extract_pdf_text(pdf_path, max_pages=15)
        full_text = "\n".join([text for _, text in pages])

        for card in cards:
            status = card.get("amount", {}).get("status", "NOT_AVAILABLE")
            if status != "UNCONFIRMED":
                continue

            canonical_name = card.get("coverage_name_canonical", "")
            if not canonical_name:
                continue

            # Simple normalization: remove brackets, spaces
            normalized_name = re.sub(r'[（）()\s]', '', canonical_name)

            # Search for coverage name + amount pattern in same context (within 200 chars)
            # Pattern: coverage name followed by amount within proximity
            coverage_pattern = re.escape(normalized_name[:10])  # First 10 chars
            amount_pattern = r'(\d{1,3}(,\d{3})*)\s*(만원|천만원|백만원|원)'

            # Search in full text
            for page_num, page_text in pages:
                page_normalized = re.sub(r'[（）()\s]', '', page_text)

                # Check if coverage name appears
                if normalized_name[:10] in page_normalized or canonical_name[:10] in page_text:
                    # Check if amount pattern appears nearby
                    matches = re.finditer(amount_pattern, page_text, re.IGNORECASE)
                    for match in matches:
                        snippet_start = max(0, match.start() - 100)
                        snippet_end = min(len(page_text), match.end() + 100)
                        snippet = page_text[snippet_start:snippet_end].replace("\n", " ").strip()

                        candidates.append({
                            "insurer": insurer,
                            "coverage_canonical": canonical_name,
                            "status": status,
                            "page": page_num,
                            "snippet": snippet,
                            "label": "MISS_CANDIDATE"
                        })
                        break  # One candidate per coverage is enough

    # Generate report
    report_lines = [
        "# Step7 Miss Candidates",
        "",
        "**Generated by**: STEP NEXT-17B",
        "**Criteria**: amount.status == UNCONFIRMED AND amount evidence found in proposal PDF",
        "",
        "## Purpose",
        "",
        "Identify cases where Step7 extraction may have missed amounts that exist in source documents.",
        "These are CANDIDATES requiring manual review before confirming as true misses.",
        "",
        f"## Summary",
        "",
        f"**Total Candidates**: {len(candidates)}",
        "",
        "## Candidates List",
        "",
    ]

    if not candidates:
        report_lines.append("No miss candidates detected. All UNCONFIRMED cases lack PDF evidence.")
    else:
        for i, cand in enumerate(candidates, 1):
            report_lines.append(f"### Candidate {i}")
            report_lines.append(f"- **Insurer**: {cand['insurer']}")
            report_lines.append(f"- **Coverage**: {cand['coverage_canonical']}")
            report_lines.append(f"- **Status**: {cand['status']}")
            report_lines.append(f"- **Page**: {cand['page']}")
            report_lines.append(f"- **Evidence Snippet**: {cand['snippet'][:200]}...")
            report_lines.append(f"- **Label**: {cand['label']}")
            report_lines.append("")

    report_lines.extend([
        "## Next Steps",
        "",
        "1. Manual review of each candidate to confirm true miss vs false positive",
        "2. If true misses confirmed, update Step7 extraction logic",
        "3. Re-run Step7 and validate against this candidate list",
        "",
        "## Important",
        "",
        "- This is detection ONLY - no Step7 logic changes in this STEP",
        "- Candidates may include false positives (unrelated amounts)",
        "- Final decision requires domain expert review",
        "",
    ])

    return "\n".join(report_lines), candidates


def main():
    """Run all audit reports for STEP NEXT-17B."""
    print("=== STEP NEXT-17B Audit Runner ===\n")

    # 1. Scan insurers
    print("1. Scanning insurers...")
    insurers = scan_insurers()
    print(f"   Found {len(insurers)} insurers: {', '.join(insurers)}\n")

    # 2. Amount Status Dashboard
    print("2. Generating Amount Status Dashboard...")
    dashboard = generate_amount_status_dashboard(insurers)
    dashboard_path = AUDIT_DIR / "AMOUNT_STATUS_DASHBOARD.md"
    with open(dashboard_path, "w", encoding="utf-8") as f:
        f.write(dashboard)
    print(f"   ✅ {dashboard_path}\n")

    # 3. Type Classification by Evidence
    print("3. Generating Type Classification Report...")
    type_report, type_results = generate_type_classification_report(insurers)
    type_path = AUDIT_DIR / "INSURER_TYPE_BY_EVIDENCE.md"
    with open(type_path, "w", encoding="utf-8") as f:
        f.write(type_report)
    print(f"   ✅ {type_path}\n")

    # 4. Type Map Diff
    print("4. Generating Type Map Diff Report...")
    diff_report = generate_type_map_diff_report(insurers, type_results)
    diff_path = AUDIT_DIR / "TYPE_MAP_DIFF_REPORT.md"
    with open(diff_path, "w", encoding="utf-8") as f:
        f.write(diff_report)
    print(f"   ✅ {diff_path}\n")

    # 5. Step7 Miss Candidates
    print("5. Detecting Step7 Miss Candidates...")
    miss_report, candidates = detect_step7_miss_candidates(insurers)
    miss_path = AUDIT_DIR / "STEP7_MISS_CANDIDATES.md"
    with open(miss_path, "w", encoding="utf-8") as f:
        f.write(miss_report)
    print(f"   ✅ {miss_path}")
    print(f"   Found {len(candidates)} miss candidates\n")

    print("=== Audit Complete ===")
    print(f"\nAll reports saved to: {AUDIT_DIR}/")
    print("\nNext: Run regression tests with `python -m pytest -q`")


if __name__ == "__main__":
    main()
